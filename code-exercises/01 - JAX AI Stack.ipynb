{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rcrowe-google/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb)\n",
        "\n",
        "# Introduction\n",
        "\n",
        "**Welcome to the JAX AI Stack Exercises!**\n",
        "\n",
        "This notebook is designed to accompany the \"Leveraging the JAX AI Stack\" lecture.  You'll get hands-on experience with core JAX concepts, Flax NNX for model building, Optax for optimization, and Orbax for checkpointing.\n",
        "\n",
        "The exercises will guide you through implementing key components, drawing parallels to PyTorch where appropriate, to solidify your understanding.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "AEYnLrsY27El"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OPA5MMD621LQ",
        "outputId": "10d6c20f-7754-4eb5-e896-8a06b07cadd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m450.6/456.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.2/456.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/499.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.3/354.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.1/523.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hJAX version: 0.7.0\n",
            "Flax version: 0.11.1\n",
            "Optax version: 0.2.5\n",
            "Orbax version: 0.11.23\n"
          ]
        }
      ],
      "source": [
        "# @title Setup: Install and Import Libraries\n",
        "# Install necessary libraries\n",
        "!pip install -q jax-ai-stack==2025.9.3\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from flax import nnx\n",
        "import optax\n",
        "import orbax.checkpoint as ocp # For Orbax\n",
        "from typing import Any, Dict, Tuple # For type hints\n",
        "\n",
        "# Helper to print PyTrees more nicely for demonstration\n",
        "import pprint\n",
        "import os # For Orbax directory management\n",
        "import shutil # For cleaning up Orbax directory\n",
        "\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Flax version: {flax.__version__}\")\n",
        "print(f\"Optax version: {optax.__version__}\")\n",
        "print(f\"Orbax version: {ocp.__version__}\")\n",
        "\n",
        "# Global JAX PRNG key for reproducibility in exercises\n",
        "# Students can learn to split this key for different operations.\n",
        "main_key = jax.random.key(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: JAX Core & NumPy API\n",
        "\n",
        "**Goal**: Get familiar with jax.numpy and JAX's functional programming style.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create two JAX arrays, a (a 2x2 matrix of random numbers) and b (a 2x2 matrix of ones) using jax.numpy (jnp). You'll need a jax.random.key for creating random numbers.\n",
        "2. Perform element-wise addition of a and b.\n",
        "3. Perform matrix multiplication of a and b.\n",
        "4. Demonstrate JAX's immutability:\n",
        " - Store the Python id() of array a.\n",
        " - Perform an operation like a = a + 1.\n",
        " - Print the new id() of a and observe that it has changed, indicating a new array was created."
      ],
      "metadata": {
        "id": "3gC7luR35tJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 1\n",
        "key_ex1, main_key = jax.random.split(main_key) # Split the main key\n",
        "\n",
        "# 1. Create JAX arrays a and b\n",
        "# TODO: Create array 'a' (2x2 random normal) and 'b' (2x2 ones)\n",
        "a = jax.random.normal(key_ex1, (2,2))\n",
        "b = jnp.ones((2,2))\n",
        "\n",
        "print(\"Array a:\\n\", a)\n",
        "print(\"Array b:\\n\", b)\n",
        "\n",
        "# 2. Perform element-wise addition\n",
        "# TODO: Add a and b\n",
        "c = a + b\n",
        "print(\"Element-wise sum c = a + b:\\n\", c)\n",
        "\n",
        "# 3. Perform matrix multiplication\n",
        "d = a @ b\n",
        "print(\"Matrix product d = a @ b:\\n\", d)\n",
        "\n",
        "# 4. Demonstrate immutability\n",
        "original_a_id = id(a)\n",
        "print(f\"Original id(a): {original_a_id}\")\n",
        "\n",
        "# TODO: Perform an operation that reassigns 'a', e.g., a = a + 1\n",
        "a_new_ref = a + 1\n",
        "new_a_id = id(a_new_ref)\n",
        "print(f\"New id(a) after 'a = a + 1': {new_a_id}\")\n",
        "\n",
        "# TODO: Check if original_a_id is different from new_a_id\n",
        "print(f\"IDs are different: {new_a_id != original_a_id}\") # Placeholder"
      ],
      "metadata": {
        "id": "8Tq_WFzc5Ycl",
        "outputId": "9f4d9afd-98e3-498a-8cdb-02d33a19a541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array a:\n",
            " [[ 1.0040143 -0.9063372]\n",
            " [-0.7481722 -1.1713669]]\n",
            "Array b:\n",
            " [[1. 1.]\n",
            " [1. 1.]]\n",
            "Element-wise sum c = a + b:\n",
            " [[ 2.0040143   0.0936628 ]\n",
            " [ 0.25182778 -0.17136693]]\n",
            "Matrix product d = a @ b:\n",
            " [[ 0.09767705  0.09767705]\n",
            " [-1.9195392  -1.9195392 ]]\n",
            "Original id(a): 274774864\n",
            "New id(a) after 'a = a + 1': 280091648\n",
            "IDs are different: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 1: JAX Core & NumPy API\n",
        "key_ex1_sol, main_key = jax.random.split(main_key)\n",
        "key_ex1_sol = key_ex1\n",
        "# 1. Create JAX arrays a and b\n",
        "a_sol = jax.random.normal(key_ex1_sol, (2, 2))\n",
        "b_sol = jnp.ones((2, 2))\n",
        "\n",
        "print(\"Array a:\\n\", a_sol)\n",
        "print(\"Array b:\\n\", b_sol)\n",
        "\n",
        "# 2. Perform element-wise addition\n",
        "c_sol = a_sol + b_sol\n",
        "print(\"Element-wise sum c = a + b:\\n\", c_sol)\n",
        "\n",
        "# 3. Perform matrix multiplication\n",
        "d_sol = jnp.dot(a_sol, b_sol) # or d = a @ b\n",
        "print(\"Matrix product d = a @ b:\\n\", d_sol)\n",
        "\n",
        "# 4. Demonstrate immutability\n",
        "original_a_id_sol = id(a_sol)\n",
        "print(f\"Original id(a_sol): {original_a_id_sol}\")\n",
        "\n",
        "a_sol_new_ref = a_sol + 1 # This creates a new array and rebinds the Python variable.\n",
        "new_a_id_sol = id(a_sol_new_ref)\n",
        "print(f\"New id(a_sol_new_ref) after 'a_sol = a_sol + 1': {new_a_id_sol}\")\n",
        "print(f\"IDs are different: {original_a_id_sol != new_a_id_sol}\")\n",
        "print(\"This shows that the original array was not modified in-place; a new array was created.\")"
      ],
      "metadata": {
        "id": "0p2HrUzH6NYQ",
        "outputId": "8b86312a-ae61-43bc-8610-ff9fb366b345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array a:\n",
            " [[ 1.0040143 -0.9063372]\n",
            " [-0.7481722 -1.1713669]]\n",
            "Array b:\n",
            " [[1. 1.]\n",
            " [1. 1.]]\n",
            "Element-wise sum c = a + b:\n",
            " [[ 2.0040143   0.0936628 ]\n",
            " [ 0.25182778 -0.17136693]]\n",
            "Matrix product d = a @ b:\n",
            " [[ 0.09767705  0.09767705]\n",
            " [-1.9195392  -1.9195392 ]]\n",
            "Original id(a_sol): 280370816\n",
            "New id(a_sol_new_ref) after 'a_sol = a_sol + 1': 280483328\n",
            "IDs are different: True\n",
            "This shows that the original array was not modified in-place; a new array was created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: jax.jit (Just-In-Time Compilation)\n",
        "\n",
        "**Goal**: Understand how to use jax.jit to compile JAX functions for performance.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Define a Python function compute_heavy_stuff(x, w, b) that performs a sequence of jnp operations:\n",
        " - y = jnp.dot(x, w)\n",
        " - y = y + b\n",
        " - y = jnp.tanh(y)\n",
        " - result = jnp.sum(y)\n",
        " - Return result.\n",
        "2. Create a JIT-compiled version of this function, fast_compute_heavy_stuff, using jax.jit.\n",
        "3. Create some large dummy JAX arrays for x, w, and b.\n",
        "4. Call both the original and JIT-compiled functions with the dummy data.\n",
        "5. (Optional) Use the `%timeit` magic command in Colab (in separate cells) to compare their execution speeds. Remember that the first call to a JIT-compiled function includes compilation time."
      ],
      "metadata": {
        "id": "MK4rErEp6WPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 2\n",
        "key_ex2_main, main_key = jax.random.split(main_key)\n",
        "key_ex2_x, key_ex2_w, key_ex2_b = jax.random.split(key_ex2_main, 3)\n",
        "\n",
        "# 1. Define the Python function\n",
        "def compute_heavy_stuff(x, w, b):\n",
        "    # TODO: Implement the operations\n",
        "    y1 = jnp.dot(x,w)\n",
        "    y2 = y1 + b\n",
        "    y3 = jnp.tanh(y2)\n",
        "    result = jnp.sum(y3)\n",
        "    return result\n",
        "\n",
        "# 2. Create a JIT-compiled version\n",
        "# TODO: Use jax.jit to compile compute_heavy_stuff\n",
        "fast_compute_heavy_stuff = jax.jit(compute_heavy_stuff)\n",
        "\n",
        "# 3. Create dummy data\n",
        "dim1, dim2, dim3 = 500, 1000, 500\n",
        "x_data = jax.random.normal(key_ex2_x, (dim1, dim2))\n",
        "w_data = jax.random.normal(key_ex2_w, (dim2, dim3))\n",
        "b_data = jax.random.normal(key_ex2_b, (dim3,))\n",
        "\n",
        "# 4. Call both functions\n",
        "result_original = compute_heavy_stuff(x_data, w_data, b_data)\n",
        "result_fast_first_call = fast_compute_heavy_stuff(x_data, w_data, b_data) # First call (compiles)\n",
        "result_fast_second_call = fast_compute_heavy_stuff(x_data, w_data, b_data) # Second call (uses compiled)\n",
        "\n",
        "print(f\"Result (original): {result_original}\")\n",
        "print(f\"Result (fast, 1st call): {result_fast_first_call}\")\n",
        "print(f\"Result (fast, 2nd call): {result_fast_second_call}\")\n",
        "\n",
        "if result_original is not None and result_fast_first_call is not None:\n",
        "  assert jnp.allclose(result_original, result_fast_first_call), \"Results should match!\"\n",
        "  print(\"\\nResults from original and JIT-compiled functions match.\")\n",
        "\n",
        "# 5. Optional: Timing (use %timeit in separate cells for accuracy)\n",
        "# print(\"\\nTo see the speed difference, run these in separate cells:\")\n",
        "# print(\"%timeit compute_heavy_stuff(x_data, w_data, b_data).block_until_ready()\")\n",
        "# print(\"%timeit fast_compute_heavy_stuff(x_data, w_data, b_data).block_until_ready()\")"
      ],
      "metadata": {
        "id": "SNwAyNyO6SM3",
        "outputId": "a0b135cd-4392-4a68-fed3-521e01bb7b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result (original): -56.06746292114258\n",
            "Result (fast, 1st call): -56.06746292114258\n",
            "Result (fast, 2nd call): -56.06746292114258\n",
            "\n",
            "Results from original and JIT-compiled functions match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 2: `jax.jit` (Just-In-Time Compilation)\n",
        "key_ex2_sol_main, main_key = jax.random.split(main_key)\n",
        "key_ex2_sol_x, key_ex2_sol_w, key_ex2_sol_b = jax.random.split(key_ex2_sol_main, 3)\n",
        "key_ex2_sol_x, key_ex2_sol_w, key_ex2_sol_b = key_ex2_x, key_ex2_w, key_ex2_b\n",
        "# 1. Define the Python function\n",
        "def compute_heavy_stuff_sol(x, w, b):\n",
        "    y = jnp.dot(x, w)\n",
        "    y = y + b\n",
        "    y = jnp.tanh(y)\n",
        "    result = jnp.sum(y)\n",
        "    return result\n",
        "\n",
        "# 2. Create a JIT-compiled version\n",
        "fast_compute_heavy_stuff_sol = jax.jit(compute_heavy_stuff_sol)\n",
        "\n",
        "# 3. Create dummy data\n",
        "dim1_sol, dim2_sol, dim3_sol = 500, 1000, 500\n",
        "x_data_sol = jax.random.normal(key_ex2_sol_x, (dim1_sol, dim2_sol))\n",
        "w_data_sol = jax.random.normal(key_ex2_sol_w, (dim2_sol, dim3_sol))\n",
        "b_data_sol = jax.random.normal(key_ex2_sol_b, (dim3_sol,))\n",
        "\n",
        "# 4. Call both functions\n",
        "# Call original once to ensure it's not timed with any JAX overhead if it were the first JAX op\n",
        "result_original_sol = compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()\n",
        "\n",
        "# First call to JITed function includes compilation time\n",
        "result_fast_sol_first_call = fast_compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()\n",
        "\n",
        "# Subsequent calls use the cached compiled code\n",
        "result_fast_sol_second_call = fast_compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()\n",
        "\n",
        "print(f\"Result (original): {result_original_sol}\")\n",
        "print(f\"Result (fast, 1st call): {result_fast_sol_first_call}\")\n",
        "print(f\"Result (fast, 2nd call): {result_fast_sol_second_call}\")\n",
        "\n",
        "assert jnp.allclose(result_original_sol, result_fast_sol_first_call), \"Results should match!\"\n",
        "print(\"\\nResults from original and JIT-compiled functions match.\")\n",
        "\n",
        "# 5. Optional: Timing\n",
        "# To accurately measure, run these in separate Colab cells:\n",
        "# Cell 1:\n",
        "# %timeit compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()\n",
        "# Cell 2:\n",
        "# %timeit fast_compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()\n",
        "# You should observe that the JIT-compiled version is significantly faster after the initial compilation.\n",
        "print(\"\\nTo see the speed difference, run the %timeit commands (provided in comments above) in separate cells.\")"
      ],
      "metadata": {
        "id": "xOLQxFay61ls",
        "outputId": "7da609a7-83e1-4d78-8688-9a00f2ca618c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result (original): -56.06746292114258\n",
            "Result (fast, 1st call): -56.06746292114258\n",
            "Result (fast, 2nd call): -56.06746292114258\n",
            "\n",
            "Results from original and JIT-compiled functions match.\n",
            "\n",
            "To see the speed difference, run the %timeit commands (provided in comments above) in separate cells.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()"
      ],
      "metadata": {
        "id": "oSql-6-VeDwj",
        "outputId": "a87e6e36-ed27-408c-d660-5c599a5a7797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.4 ms ± 2.68 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit fast_compute_heavy_stuff_sol(x_data_sol, w_data_sol, b_data_sol).block_until_ready()"
      ],
      "metadata": {
        "id": "G6PfjULWeNZY",
        "outputId": "949f9d80-a8ec-43ba-d0a2-4ea93f4a6c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.6 ms ± 6.97 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: jax.grad (Automatic Differentiation)\n",
        "\n",
        "**Goal**: Learn to use jax.grad to compute gradients of functions.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Define a Python function scalar_loss(params, x, y_true) that:\n",
        " - Takes a dictionary params with keys 'w' and 'b'.\n",
        " - Computes y_pred = params['w'] * x + params['b'].\n",
        " - Returns a scalar loss, e.g., jnp.mean((y_pred - y_true)**2).\n",
        "2. Use jax.grad to create a new function, compute_gradients, that computes the gradient of scalar_loss with respect to its first argument (params).\n",
        "3. Initialize some dummy params, x_input, and y_target values.\n",
        "4. Call compute_gradients to get the gradients. Print the gradients."
      ],
      "metadata": {
        "id": "MNZqLNB57CpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 3\n",
        "\n",
        "# 1. Define the scalar_loss function\n",
        "def scalar_loss(params: Dict[str, jnp.ndarray], x: jnp.ndarray, y_true: jnp.ndarray) -> jnp.ndarray:\n",
        "    multiply = params['w'] * x\n",
        "    # jax.debug.print(\"The values are: {m}, {dot}\", m=multiply, dot=jnp.dot(params['w'], x))\n",
        "    # y_pred = jnp.dot(params['w'], x) + params['b']\n",
        "    y_pred = params['w'] * x + params['b']\n",
        "    loss = jnp.mean((y_pred - y_true)**2)  # MSE\n",
        "    # Grads: https://docs.google.com/document/d/1dn7kQdb0ylA3fThPyEqxN-fTqKoGFTExn2cSVgnxNCY/edit?resourcekey=0-bWI4umq5Ywg6sw-pnOi_3w&tab=t.0\n",
        "    return loss\n",
        "\n",
        "# 2. Create the gradient function using jax.grad\n",
        "compute_gradients = jax.grad(scalar_loss)\n",
        "\n",
        "# 3. Initialize dummy data\n",
        "params_init = {'w': jnp.array(2.0), 'b': jnp.array(1.0)}\n",
        "x_input_data = jnp.array([1.0, 2.0, 3.0])\n",
        "y_target_data = jnp.array([7.0, 9.0, 11.0]) # Targets for y = 3x + 4 (to make non-zero loss with init_params)\n",
        "\n",
        "# 4. Call the gradient function\n",
        "# Placeholder compute_gradients(params_init, x_input_data, y_target_data)\n",
        "gradients = compute_gradients(params_init, x_input_data, y_target_data)\n",
        "print(\"Initial params:\", params_init)\n",
        "print(\"Gradients w.r.t params:\\n\", gradients)\n",
        "\n",
        "# Expected gradients (manual calculation for y_pred = wx+b, loss = mean((y_pred - y_true)^2)):\n",
        "# dL/dw = mean(2 * (wx+b - y_true) * x)\n",
        "# dL/db = mean(2 * (wx+b - y_true) * 1)\n",
        "# For params_init={'w': 2.0, 'b': 1.0}, x=[1,2,3], y_true=[7,9,11]\n",
        "# x=1: y_pred = 2*1+1 = 3. Error = 3-7 = -4. dL/dw_i_term = 2*(-4)*1 = -8.  dL/db_i_term = 2*(-4)*1 = -8\n",
        "# x=2: y_pred = 2*2+1 = 5. Error = 5-9 = -4. dL/dw_i_term = 2*(-4)*2 = -16. dL/db_i_term = 2*(-4)*1 = -8\n",
        "# x=3: y_pred = 2*3+1 = 7. Error = 7-11 = -4. dL/dw_i_term = 2*(-4)*3 = -24. dL/db_i_term = 2*(-4)*1 = -8\n",
        "# Mean gradients: dL/dw = (-8-16-24)/3 = -48/3 = -16.  dL/db = (-8-8-8)/3 = -24/3 = -8.\n",
        "if gradients is not None:\n",
        "    assert jnp.isclose(gradients['w'], -16.0)\n",
        "    assert jnp.isclose(gradients['b'], -8.0)\n",
        "    print(\"\\nGradients match expected values.\")"
      ],
      "metadata": {
        "id": "g8S-6snP69KI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9371792f-0c13-4997-8967-473558a83f60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial params: {'w': Array(2., dtype=float32, weak_type=True), 'b': Array(1., dtype=float32, weak_type=True)}\n",
            "Gradients w.r.t params:\n",
            " {'b': Array(-8., dtype=float32, weak_type=True), 'w': Array(-16., dtype=float32, weak_type=True)}\n",
            "\n",
            "Gradients match expected values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how the partial derivative $\\frac{\\partial L}{\\partial w}$ results in $-16.0$, we must apply the **Chain Rule** to the specific functions and data provided in **Exercise 3** of the notebook.\n",
        "\n",
        "### 1. The Functions\n",
        "In the exercise, the model and loss are defined as follows ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=x+params+b+y_pred+params+w+x+params+b+loss+jnp+mean+y_pred+y_true+2#scrollTo=OPA5MMD621LQ)):\n",
        "*   **Prediction**: $\\hat{y} = w \\cdot x + b$\n",
        "*   **Loss (Mean Squared Error)**: $L = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$\n",
        "\n",
        "### 2. Applying the Chain Rule\n",
        "As noted in the notebook's summary of optimization concepts, the Chain Rule allows us to decompose the derivative of the total loss into a product of simpler derivatives ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=the+chain+rule+allows+us+to+break+down+the+derivative+of+the+total+loss+into+a+product+of+simpler+derivatives+from+each+step+of+the+model#scrollTo=OPA5MMD621LQ)):\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$ ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=frac+partial+l+partial+w+frac+partial+l+partial+hat+y+cdot+frac+partial+hat+y+partial+w#scrollTo=OPA5MMD621LQ))\n",
        "\n",
        "1.  **Differentiating the Loss**: $\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{n} \\sum 2(\\hat{y}_i - y_i)$\n",
        "2.  **Differentiating the Prediction**: $\\frac{\\partial \\hat{y}}{\\partial w} = x$\n",
        "\n",
        "Combining these results in the gradient formula used for verification in the notebook ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=dl+dw+mean+2+wx+b+y_true+x#scrollTo=OPA5MMD621LQ)):\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} 2(w \\cdot x_i + b - y_i) \\cdot x_i $$\n",
        "\n",
        "### 3. Plugging in the Data\n",
        "The notebook initializes the following values ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=params_init+w+jnp+array+2+0+b+jnp+array+1+0+x_input_data+jnp+array+1+0+2+0+3+0+y_target_data+jnp+array+7+0+9+0+11+0#scrollTo=OPA5MMD621LQ)):\n",
        "*   **Params**: $w = 2.0, b = 1.0$\n",
        "*   **Data**: $x = [1, 2, 3], y = [7, 9, 11]$\n",
        "\n",
        "Calculating the individual terms ($2 \\cdot (\\text{error}) \\cdot x$) for each data point ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=y_true+7+9+11+x+1+y_pred+21+1+3+error+3+7+4+dl+dw_i_term+2+4+1+8#scrollTo=OPA5MMD621LQ)):\n",
        "*   **For $x=1$**: $2 \\cdot (2(1) + 1 - 7) \\cdot 1 = 2 \\cdot (-4) \\cdot 1 = -8$\n",
        "*   **For $x=2$**: $2 \\cdot (2(2) + 1 - 9) \\cdot 2 = 2 \\cdot (-4) \\cdot 2 = -16$ ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=x+2+y_pred+22+1+5+error+5+9+4+dl+dw_i_term+2+4+2+16#scrollTo=OPA5MMD621LQ))\n",
        "*   **For $x=3$**: $2 \\cdot (2(3) + 1 - 11) \\cdot 3 = 2 \\cdot (-4) \\cdot 3 = -24$ ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=x+3+y_pred+23+1+7+error+7+11+4+dl+dw_i_term+2+4+3+24#scrollTo=OPA5MMD621LQ))\n",
        "\n",
        "### 4. Final Calculation\n",
        "The partial derivative is the average (mean) of these three terms ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=mean+gradients+dl+dw+8+16+24+3+48+3+16#scrollTo=OPA5MMD621LQ)):\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w} = \\frac{-8 + (-16) + (-24)}{3} = \\frac{-48}{3} = -16.0 $$\n",
        "\n",
        "This explains how the manual calculation matches the result produced by `jax.grad` in the exercise ([source](https://colab.sandbox.google.com/github/Obliviour/Learning-JAX/blob/main/code-exercises/01%20-%20JAX%20AI%20Stack.ipynb?content_ref=assert+jnp+isclose+gradients+b+8+0+print+ngradients+match+expected+values#scrollTo=OPA5MMD621LQ)).\n"
      ],
      "metadata": {
        "id": "i4GXKcWHrl7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Machine Learning Optimization Concepts\n",
        "Topic: Machine Learning Foundations & Optimization\n",
        "Summary Date: 2025-12-31\n",
        "Overview\n",
        "This document summarizes our discussion regarding the fundamental mechanics of training machine learning models, specifically focusing on how loss functions, gradients, and optimization algorithms work together to enable learning.\n",
        "Loss Functions & Mean Squared Error (MSE)\n",
        "The scalar_loss function is a critical component that calculates a single numerical value representing the model's error. In many linear regression tasks, this is implemented using Mean Squared Error (MSE).\n",
        "MSE Formula: $$ \\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}_i)^2 $$\n",
        "\n",
        "Why Square the Error?\n",
        "Squaring the error serves several mathematical and practical purposes:\n",
        "1. Prevents Cancellation: Raw errors can be positive or negative; squaring ensures they all contribute positively to the total loss.\n",
        "2. Penalizes Outliers: Squaring amplifies larger errors, making the model more sensitive to significant mispredictions.\n",
        "3. Mathematical Optimization: The derivative of a squared term is linear and smooth, making it easier for optimization algorithms to solve.\n",
        "Gradients & The Chain Rule\n",
        "A gradient is a vector of partial derivatives that indicates the direction and magnitude of the steepest increase in loss. To find how specific weights impact the final loss in a complex, multi-layered model, we use the Chain Rule.\n",
        "The Chain Rule allows us to break down the derivative of the total loss into a product of simpler derivatives from each step of the model:\n",
        "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$\n",
        "Backpropagation\n",
        "Backpropagation is the efficient algorithm used to apply the Chain Rule across all layers of a neural network. It involves a backward pass from the output (loss) back to the inputs (weights).\n",
        "Key points:\n",
        "- Efficiency: It reuses intermediate calculations, making it computationally as cheap as the forward pass.\n",
        "- Credit Assignment: It determines exactly which weight \"knobs\" need to be adjusted and by how much to reduce the global error.\n",
        "- Shape Matching: The resulting gradient matrix always matches the shape of the weight matrix (e.g., embed_dim x ff_dim).\n",
        "The Role of the Learning Rate\n",
        "The Learning Rate ($\\alpha$) is a hyperparameter that scales the gradient before it is used to update the weights.\n",
        "Update Rule: $$ w_{new} = w_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w} $$\n",
        "\n",
        "The Learning Rate acts as a \"volume knob\" that prevents raw gradients from being too large and destabilizing the model. It also bridges the units between the loss function and the weight parameters.\n",
        "Summary of Optimization Logic\n",
        "Component\n",
        "Function\n",
        "\n",
        "MSE Loss\n",
        "A single scalar \"score\" of the model's total error.\n",
        "Gradients\n",
        "A matrix telling the model how each individual weight scalar contributes to the global loss.\n",
        "Learning Rate\n",
        "Scales the gradient into a \"small amount\" that the weight can absorb safely.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oUCBFgYUgRqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In JAX, `jnp.dot` and `jnp.matmul` are both used for array multiplication, but they differ significantly in their handling of scalars and higher-dimensional batch data .\n",
        "\n",
        "### 1. Scalar Multiplication\n",
        "*   **`jnp.dot`**: If either input is a scalar, it performs element-wise multiplication, which is equivalent to `jnp.multiply` .\n",
        "*   **`jnp.matmul`**: This function does not allow scalar inputs and will raise an error if one is provided .\n",
        "\n",
        "### 2. Higher-Dimensional (N-D) Behavior\n",
        "The primary mathematical difference lies in how \"batch\" dimensions are handled when arrays have more than two dimensions .\n",
        "\n",
        "#### **Stacking (`jnp.dot`)**\n",
        "`jnp.dot` is a **stacking operation** because it combines the leading dimensions of both inputs into the final output shape . It performs a sum-product over the last axis of the first array and the second-to-last axis of the second array, while \"stacking\" every other dimension as an independent axis .\n",
        "\n",
        "*   **Example**: If $a$ has shape $(3, 2, 4)$ and $b$ has shape $(3, 4, 1)$, `jnp.dot(a, b)` results in a shape of $(3, 2, 3, 1)$. It computes every possible pairing between the batch items in $a$ and $b$.\n",
        "\n",
        "#### **Broadcasting (`jnp.matmul`)**\n",
        "`jnp.matmul` (invoked via the `@` operator) follows standard **broadcasting rules** to align batch dimensions . It treats batch dimensions as a single shared index that must be broadcast-compatible between the two arrays .\n",
        "\n",
        "*   **Example**: Using the same shapes as above, `jnp.matmul(a, b)` broadcasts the shared batch dimension of $3$, resulting in an output shape of $(3, 2, 1)$ .\n",
        "\n",
        "### Summary Comparison Table\n",
        "\n",
        "| Feature | `jnp.dot` (Stacking) | `jnp.matmul` (Broadcasting) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Scalars** | Acts as element-wise scaling  | Results in an error  |\n",
        "| **Batch Handling** | Stacks leading dimensions | Aligns batch dimensions via broadcasting |\n",
        "| **Common Use Case** | Model ensembles, cross-attention | Standard layer weights applied to a batch |\n"
      ],
      "metadata": {
        "id": "3McAKl_MgUE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 3: `jax.grad` (Automatic Differentiation)\n",
        "\n",
        "# 1. Define the scalar_loss function\n",
        "def scalar_loss_sol(params: Dict[str, jnp.ndarray], x: jnp.ndarray, y_true: jnp.ndarray) -> jnp.ndarray:\n",
        "    y_pred = params['w'] * x + params['b']\n",
        "    loss = jnp.mean((y_pred - y_true)**2)\n",
        "    return loss\n",
        "\n",
        "# 2. Create the gradient function using jax.grad\n",
        "# Gradient of scalar_loss w.r.t. 'params' (which is the 0-th argument)\n",
        "compute_gradients_sol = jax.grad(scalar_loss_sol, argnums=0)\n",
        "\n",
        "# 3. Initialize dummy data\n",
        "params_init_sol = {'w': jnp.array(2.0), 'b': jnp.array(1.0)}\n",
        "x_input_data_sol = jnp.array([1.0, 2.0, 3.0])\n",
        "y_target_data_sol = jnp.array([7.0, 9.0, 11.0])\n",
        "\n",
        "# 4. Call the gradient function\n",
        "gradients_sol = compute_gradients_sol(params_init_sol, x_input_data_sol, y_target_data_sol)\n",
        "print(\"Initial params:\", params_init_sol)\n",
        "print(\"Gradients w.r.t params:\\n\", pprint.pformat(gradients_sol))\n",
        "\n",
        "# Verify with expected values (calculated in instructions)\n",
        "expected_dL_dw = -16.0\n",
        "expected_dL_db = -8.0\n",
        "assert jnp.isclose(gradients_sol['w'], expected_dL_dw), f\"Grad w.r.t 'w' is {gradients_sol['w']}, expected {expected_dL_dw}\"\n",
        "assert jnp.isclose(gradients_sol['b'], expected_dL_db), f\"Grad w.r.t 'b' is {gradients_sol['b']}, expected {expected_dL_db}\"\n",
        "print(\"\\nGradients match expected values.\")"
      ],
      "metadata": {
        "id": "jcjiql4O7ZQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a930bfb-fbc8-4871-b8f5-f46dbca0e530"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial params: {'w': Array(2., dtype=float32, weak_type=True), 'b': Array(1., dtype=float32, weak_type=True)}\n",
            "Gradients w.r.t params:\n",
            " {'b': Array(-8., dtype=float32, weak_type=True),\n",
            " 'w': Array(-16., dtype=float32, weak_type=True)}\n",
            "\n",
            "Gradients match expected values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: jax.vmap (Automatic Vectorization)\n",
        "\n",
        "**Goal**: Use jax.vmap to automatically batch operations.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Define a function apply_affine(vector, matrix, bias) that takes a single 1D vector, a 2D matrix, and a 1D bias. It should compute jnp.dot(matrix, vector) + bias.\n",
        "2. You have a batch of vectors (a 2D array where each row is a vector), but a single matrix and a single bias that should be applied to each vector in the batch.\n",
        "3. Use jax.vmap to create batched_apply_affine that efficiently applies apply_affine to each vector in the batch.\n",
        " - Hint: in_axes for jax.vmap should specify 0 for the batched vector argument, and None for matrix and bias as they are not batched (broadcasted). The out_axes should be 0 to indicate the output is batched along the first axis.\n",
        "4. Test batched_apply_affine with sample data."
      ],
      "metadata": {
        "id": "XWoB6bD-7g2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 4\n",
        "key_ex4_main, main_key = jax.random.split(main_key)\n",
        "key_ex4_vec, key_ex4_mat, key_ex4_bias = jax.random.split(key_ex4_main, 3)\n",
        "\n",
        "# 1. Define apply_affine for a single vector\n",
        "def apply_affine(vector: jnp.ndarray, matrix: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
        "    result = jnp.dot(matrix, vector) + bias\n",
        "    return result\n",
        "\n",
        "# 2. Prepare data\n",
        "batch_size = 4\n",
        "input_features = 3\n",
        "output_features = 2\n",
        "\n",
        "# batch_of_vectors: (batch_size, input_features)\n",
        "# single_matrix: (output_features, input_features)\n",
        "# single_bias: (output_features,)\n",
        "batch_of_vectors = jax.random.normal(key_ex4_vec, (batch_size, input_features))\n",
        "single_matrix = jax.random.normal(key_ex4_mat, (output_features, input_features))\n",
        "single_bias = jax.random.normal(key_ex4_bias, (output_features,))\n",
        "\n",
        "\n",
        "# 3. Use jax.vmap to create batched_apply_affine\n",
        "# TODO: Specify in_axes correctly: vector is batched, matrix and bias are not. out_axes should be 0.\n",
        "batched_apply_affine = jax.vmap(apply_affine, in_axes=(0, None, None), out_axes=0)\n",
        "\n",
        "\n",
        "# 4. Test batched_apply_affine\n",
        "result_vmap = batched_apply_affine(batch_of_vectors, single_matrix, single_bias)\n",
        "print(\"Batch of vectors shape:\", batch_of_vectors.shape)\n",
        "print(\"Single matrix shape:\", single_matrix.shape)\n",
        "print(\"Single bias shape:\", single_bias.shape)\n",
        "if result_vmap is not None:\n",
        "    print(\"Result using vmap shape:\", result_vmap.shape) # Expected: (batch_size, output_features)\n",
        "\n",
        "    # For comparison, a manual loop (less efficient):\n",
        "    manual_results = []\n",
        "    for i in range(batch_size):\n",
        "        manual_results.append(apply_affine(batch_of_vectors[i], single_matrix, single_bias))\n",
        "    result_manual_loop = jnp.stack(manual_results)\n",
        "    assert jnp.allclose(result_vmap, result_manual_loop)\n",
        "    print(\"vmap result matches manual loop result.\")\n",
        "else:\n",
        "    print(\"result_vmap is None.\")"
      ],
      "metadata": {
        "id": "vA9mu1si7dii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9074619-4953-483c-b715-ff9c7c14b577"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of vectors shape: (4, 3)\n",
            "Single matrix shape: (2, 3)\n",
            "Single bias shape: (2,)\n",
            "Result using vmap shape: (4, 2)\n",
            "vmap result matches manual loop result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In JAX, `jax.vmap` (vectorizing map) is a transformation that automatically converts a function designed for a single data point into one that processes batches of data efficiently\n",
        "\n",
        "### How `in_axes` and `out_axes` Work\n",
        "To answer your specific question: the axes you provide in `in_axes` are the **batched axes** (the dimensions you want to map over). If you want an argument to be **non-batched** (broadcasted across the batch), you provide `None`\n",
        "\n",
        "*   **`in_axes`**: A tuple or list where each element corresponds to a function argument.\n",
        "    *   **Integer (e.g., `0`)**: Tells JAX that this argument has a batch dimension at that index and should be mapped over\n",
        "    *   **`None`**: Tells JAX that this argument does **not** have a batch dimension and should be treated as a single, shared value for all items in the batch\n",
        "*   **`out_axes`**: Specifies where the resulting batch dimension should appear in the output array (usually `0` for the leading axis)\n",
        "\n",
        "### Example: Batched Predictions\n",
        "Consider a function `predict(params, x)` that calculates the dot product for a single vector. You want to apply it to a single set of `params` but a **batch** of $100$ input vectors.\n",
        "\n",
        "```python\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def predict(params, input_vector):\n",
        "    return jnp.dot(params, input_vector)\n",
        "\n",
        "# Single matrix of params and a batch of 100 vectors\n",
        "params = jnp.ones((5, 10))\n",
        "batch_of_vectors = jnp.ones((100, 10))\n",
        "\n",
        "# Vectorize the function\n",
        "# in_axes=(None, 0) means:\n",
        "# - params: None (Non-batched/Broadcast)\n",
        "# - input_vector: 0 (Batched along axis 0)\n",
        "batched_predict = jax.vmap(predict, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "# Run the batched version\n",
        "predictions = batched_predict(params, batch_of_vectors)\n",
        "print(predictions.shape)  # Output: (100, 5)\n",
        "```\n",
        "\n",
        "\n",
        "### Summary of Axis Specification\n",
        "*   **To batch an argument**: Provide the index of the batch dimension (e.g., `0`, `1`, etc.)\n",
        "*   **To keep an argument non-batched**: Provide `None`\n",
        "*   **Default**: If you don't specify `in_axes`, JAX assumes all arguments are batched along axis `0`\n",
        "\n",
        "One common \"gotcha\" is that keyword arguments are **always** batched along their leading axis by default. If you need a keyword argument to be non-batched, you should use `functools.partial` to fix that value before calling `vmap`\n"
      ],
      "metadata": {
        "id": "jQmqawW_yrUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 4: `jax.vmap` (Automatic Vectorization)\n",
        "key_ex4_sol_main, main_key = jax.random.split(main_key)\n",
        "key_ex4_sol_vec, key_ex4_sol_mat, key_ex4_sol_bias = jax.random.split(key_ex4_sol_main, 3)\n",
        "\n",
        "# 1. Define apply_affine for a single vector\n",
        "def apply_affine_sol(vector: jnp.ndarray, matrix: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
        "    return jnp.dot(matrix, vector) + bias\n",
        "\n",
        "# 2. Prepare data\n",
        "batch_size_sol = 4\n",
        "input_features_sol = 3\n",
        "output_features_sol = 2\n",
        "\n",
        "batch_of_vectors_sol = jax.random.normal(key_ex4_sol_vec, (batch_size_sol, input_features_sol))\n",
        "single_matrix_sol = jax.random.normal(key_ex4_sol_mat, (output_features_sol, input_features_sol))\n",
        "single_bias_sol = jax.random.normal(key_ex4_sol_bias, (output_features_sol,))\n",
        "\n",
        "# 3. Use jax.vmap to create batched_apply_affine\n",
        "# Vector is batched along axis 0, matrix and bias are not batched (broadcasted).\n",
        "# out_axes=0 means the output will also be batched along its first axis.\n",
        "batched_apply_affine_sol = jax.vmap(apply_affine_sol, in_axes=(0, None, None), out_axes=0)\n",
        "\n",
        "# 4. Test batched_apply_affine\n",
        "result_vmap_sol = batched_apply_affine_sol(batch_of_vectors_sol, single_matrix_sol, single_bias_sol)\n",
        "print(\"Batch of vectors shape:\", batch_of_vectors_sol.shape)\n",
        "print(\"Single matrix shape:\", single_matrix_sol.shape)\n",
        "print(\"Single bias shape:\", single_bias_sol.shape)\n",
        "print(\"Result using vmap shape:\", result_vmap_sol.shape) # Expected: (batch_size, output_features)\n",
        "assert result_vmap_sol.shape == (batch_size_sol, output_features_sol)\n",
        "\n",
        "# For comparison, a manual loop (less efficient):\n",
        "manual_results_sol = []\n",
        "for i in range(batch_size_sol):\n",
        "    manual_results_sol.append(apply_affine_sol(batch_of_vectors_sol[i], single_matrix_sol, single_bias_sol))\n",
        "result_manual_loop_sol = jnp.stack(manual_results_sol)\n",
        "\n",
        "assert jnp.allclose(result_vmap_sol, result_manual_loop_sol)\n",
        "print(\"\\nvmap result matches manual loop result, demonstrating correct vectorization.\")"
      ],
      "metadata": {
        "id": "q1QkKEtF76yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad30d73-3e78-46f4-8d57-3220a01d11c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of vectors shape: (4, 3)\n",
            "Single matrix shape: (2, 3)\n",
            "Single bias shape: (2,)\n",
            "Result using vmap shape: (4, 2)\n",
            "\n",
            "vmap result matches manual loop result, demonstrating correct vectorization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Flax NNX - Defining a Model\n",
        "\n",
        "**Goal**: Learn to define a simple neural network model using Flax NNX.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Define a Flax NNX model class SimpleNNXModel that inherits from nnx.Module.\n",
        "2. In its __init__, define one nnx.Linear layer. The layer should take din (input features) and dout (output features) as arguments. Remember to pass the rngs argument to nnx.Linear for parameter initialization (e.g., rngs=rngs).\n",
        "3. Implement the __call__ method (the forward pass) which takes an input x and passes it through the linear layer.\n",
        "4. Instantiate your SimpleNNXModel. You'll need to create an nnx.Rngs object using a JAX PRNG key (e.g., nnx.Rngs(params=jax.random.key(seed))). The key name params is conventional for nnx.Linear.\n",
        "5. Test your model instance with a dummy input batch. Print the output and the model's state (parameters) using nnx.display()."
      ],
      "metadata": {
        "id": "3LAlhdzq8D_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 5\n",
        "key_ex5_model_init, main_key = jax.random.split(main_key)\n",
        "\n",
        "# 1. & 2. & 3. Define the SimpleNNXModel\n",
        "class SimpleNNXModel(nnx.Module):\n",
        "    def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
        "        # TODO: Define an nnx.Linear layer named 'dense_layer'\n",
        "        # self.dense_layer = nnx.Linear(...)\n",
        "        self.some_attribute = None # Placeholder, remove later\n",
        "        pass # Remove this placeholder if class is not empty\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        # TODO: Pass input x through the dense_layer\n",
        "        # return self.dense_layer(x)\n",
        "        return x # Placeholder\n",
        "\n",
        "# 4. Instantiate the model\n",
        "model_din = 3\n",
        "model_dout = 2\n",
        "# TODO: Create nnx.Rngs for parameter initialization. Use 'params' as the key name.\n",
        "model_rngs = None # Placeholder nnx.Rngs(params=key_ex5_model_init)\n",
        "my_model = None # Placeholder SimpleNNXModel(din=model_din, dout=model_dout, rngs=model_rngs)\n",
        "\n",
        "# 5. Test with dummy data\n",
        "dummy_batch_size = 4\n",
        "dummy_input_ex5 = jnp.ones((dummy_batch_size, model_din))\n",
        "\n",
        "model_output = None # Placeholder\n",
        "if my_model is not None:\n",
        "    model_output = my_model(dummy_input_ex5)\n",
        "    print(f\"Model output shape: {model_output.shape}\")\n",
        "    print(f\"Model output:\\n{model_output}\")\n",
        "\n",
        "    model_state = my_model.get_state()\n",
        "    print(f\"\\nModel state (parameters, etc.):\")\n",
        "    pprint.pprint(model_state)\n",
        "else:\n",
        "    print(\"my_model is None.\")"
      ],
      "metadata": {
        "id": "BzUjMHll7--R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 5: Flax NNX - Defining a Model\n",
        "key_ex5_sol_model_init, main_key = jax.random.split(main_key)\n",
        "\n",
        "# 1. & 2. & 3. Define the SimpleNNXModel\n",
        "class SimpleNNXModel_Sol(nnx.Module): # Renamed for solution cell\n",
        "    def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
        "        # nnx.Linear will use the 'params' key from rngs by default for its parameters\n",
        "        self.dense_layer = nnx.Linear(din, dout, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "        return self.dense_layer(x)\n",
        "\n",
        "# 4. Instantiate the model\n",
        "model_din_sol = 3\n",
        "model_dout_sol = 2\n",
        "# Create nnx.Rngs for parameter initialization.\n",
        "# 'params' is the default key nnx.Linear looks for in the rngs object.\n",
        "model_rngs_sol = nnx.Rngs(params=key_ex5_sol_model_init)\n",
        "my_model_sol = SimpleNNXModel_Sol(din=model_din_sol, dout=model_dout_sol, rngs=model_rngs_sol)\n",
        "\n",
        "# 5. Test with dummy data\n",
        "dummy_batch_size_sol = 4\n",
        "dummy_input_ex5_sol = jnp.ones((dummy_batch_size_sol, model_din_sol))\n",
        "\n",
        "model_output_sol = my_model_sol(dummy_input_ex5_sol)\n",
        "print(f\"Model output shape: {model_output_sol.shape}\")\n",
        "print(f\"Model output:\\n{model_output_sol}\")\n",
        "\n",
        "# model_state_sol = my_model_sol.get_state()\n",
        "_, model_state_sol = nnx.split(my_model_sol)\n",
        "print(f\"\\nModel state (parameters, etc.):\")\n",
        "nnx.display(model_state_sol)\n",
        "\n",
        "# Check that parameters are present\n",
        "assert 'dense_layer' in model_state_sol, \"Key 'dense_layer' not in model_state\"\n",
        "assert 'kernel' in model_state_sol['dense_layer'], \"Key 'kernel' not in model_state['dense_layer']\"\n",
        "assert 'bias' in model_state_sol['dense_layer'], \"Key 'bias' not in model_state['dense_layer']\"\n",
        "print(\"\\nModel parameters (kernel and bias for dense_layer) are present in the state.\")"
      ],
      "metadata": {
        "id": "QbBqSZse8V9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Optax & Flax NNX - Creating an Optimizer\n",
        "\n",
        "**Goal**: Set up an Optax optimizer and wrap it with nnx.Optimizer for use with a Flax NNX model.\n",
        "\n",
        "### Instructions:\n",
        "1. Use the SimpleNNXModel_Sol class and an instance my_model_sol from the previous exercise's solution. (If running standalone, re-instantiate it).\n",
        "2. Create an Optax optimizer, for example, optax.adam with a learning rate of 0.001.\n",
        "3. Create an nnx.Optimizer instance. This wrapper links the Optax optimizer with your Flax NNX model (my_model_sol).\n",
        "4. Print the nnx.Optimizer instance and its state attribute to see the initialized optimizer state (e.g., Adam's momentum terms)."
      ],
      "metadata": {
        "id": "i4kuv2IH-FbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 6\n",
        "\n",
        "# 1. Assume my_model_sol is available from Exercise 5 solution\n",
        "# (If running standalone, re-instantiate it)\n",
        "if 'my_model_sol' not in globals():\n",
        "    print(\"Re-initializing model from Ex5 solution for Ex6.\")\n",
        "    key_ex6_model_init, main_key = jax.random.split(main_key)\n",
        "    _model_din_ex6 = 3\n",
        "    _model_dout_ex6 = 2\n",
        "    _model_rngs_ex6 = nnx.Rngs(params=key_ex6_model_init)\n",
        "    # Use solution class name if defined, otherwise student's class name\n",
        "    _ModelClass = SimpleNNXModel_Sol if 'SimpleNNXModel_Sol' in globals() else SimpleNNXModel\n",
        "    model_for_opt = _ModelClass(din=_model_din_ex6, dout=_model_dout_ex6, rngs=_model_rngs_ex6)\n",
        "    print(\"Model for optimizer created.\")\n",
        "else:\n",
        "    model_for_opt = my_model_sol # Use the one from previous solution\n",
        "    print(\"Using model 'my_model_sol' from previous exercise for 'model_for_opt'.\")\n",
        "\n",
        "\n",
        "# 2. Create an Optax optimizer\n",
        "learning_rate = 0.001\n",
        "# TODO: Create an optax.adam optimizer transform\n",
        "optax_tx = None # Placeholder optax.adam(...)\n",
        "\n",
        "# 3. Create an nnx.Optimizer wrapper\n",
        "# TODO: Wrap the model (model_for_opt) and the optax transform (optax_tx)\n",
        "# The `wrt` argument is now required to specify what to differentiate with respect to.\n",
        "nnx_optimizer = None # Placeholder nnx.Optimizer(...)\n",
        "\n",
        "# 4. Print the optimizer and its state\n",
        "print(\"\\nFlax NNX Optimizer wrapper:\")\n",
        "nnx.display(nnx_optimizer)\n",
        "\n",
        "print(\"\\nInitial Optimizer State (Optax state, e.g., Adam's momentum):\")\n",
        "if nnx_optimizer is not None and hasattr(nnx_optimizer, 'opt_state'):\n",
        "   pprint.pprint(nnx_optimizer.state)\n",
        "   # if hasattr(nnx_optimizer, 'opt_state'):\n",
        "   #     adam_state = nnx_optimizer.opt_state\n",
        "   #     assert len(adam_state) > 0 and hasattr(adam_state[0], 'count')\n",
        "   #     print(\"\\nOptimizer state structure looks plausible for Adam.\")\n",
        "else:\n",
        "   print(\"nnx_optimizer or its state is None or not structured as expected.\")"
      ],
      "metadata": {
        "id": "ytaIj3xK8ZMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 6: Optax & Flax NNX - Creating an Optimizer\n",
        "\n",
        "# 1. Use my_model_sol from Exercise 5 solution\n",
        "# If not run sequentially, ensure my_model_sol is defined:\n",
        "if 'my_model_sol' not in globals():\n",
        "    print(\"Re-initializing model from Ex5 solution for Ex6.\")\n",
        "    key_ex6_sol_model_init, main_key = jax.random.split(main_key)\n",
        "    _model_din_sol_ex6 = 3\n",
        "    _model_dout_sol_ex6 = 2\n",
        "    _model_rngs_sol_ex6 = nnx.Rngs(params=key_ex6_sol_model_init)\n",
        "    # Ensure SimpleNNXModel_Sol is used\n",
        "    my_model_sol = SimpleNNXModel_Sol(din=_model_din_sol_ex6, dout=_model_dout_sol_ex6, rngs=_model_rngs_sol_ex6)\n",
        "    print(\"Model for optimizer re-created as 'my_model_sol'.\")\n",
        "else:\n",
        "    print(\"Using model 'my_model_sol' from previous exercise.\")\n",
        "\n",
        "\n",
        "# 2. Create an Optax optimizer\n",
        "learning_rate_sol = 0.001\n",
        "# Create an optax.adam optimizer transform\n",
        "optax_tx_sol = optax.adam(learning_rate=learning_rate_sol)\n",
        "\n",
        "# 3. Create an nnx.Optimizer wrapper\n",
        "# This links the model and the Optax optimizer.\n",
        "# The optimizer state will be initialized based on the model's parameters.\n",
        "nnx_optimizer_sol = nnx.Optimizer(my_model_sol, optax_tx_sol, wrt=nnx.Param)\n",
        "\n",
        "# 4. Print the optimizer and its state\n",
        "print(\"\\nFlax NNX Optimizer wrapper:\")\n",
        "nnx.display(nnx_optimizer_sol) # Shows the model it's associated with and the Optax transform\n",
        "\n",
        "print(\"\\nInitial Optimizer State (Optax state, e.g., Adam's momentum):\")\n",
        "# nnx.Optimizer stores the actual Optax state in its .opt_state attribute.\n",
        "# This state is a PyTree that matches the structure of the model's parameters.\n",
        "pprint.pprint(nnx_optimizer_sol.opt_state)\n",
        "\n",
        "# Verify the structure of the optimizer state for Adam (count, mu, nu for each param)\n",
        "assert hasattr(nnx_optimizer_sol, 'opt_state'), \"Optax opt_state not found in nnx.Optimizer\"\n",
        "# The opt_state is a tuple, typically (CountState(), ScaleByAdamState()) for adam\n",
        "adam_optax_internal_state = nnx_optimizer_sol.opt_state\n",
        "assert len(adam_optax_internal_state) > 0 and hasattr(adam_optax_internal_state[0], 'count'), \"Adam 'count' state not found.\"\n",
        "# The second element of the tuple is often where parameter-specific states like mu and nu reside\n",
        "if len(adam_optax_internal_state) > 1 and hasattr(adam_optax_internal_state[1], 'mu'):\n",
        "    param_specific_state = adam_optax_internal_state[1]\n",
        "    assert 'dense_layer' in param_specific_state.mu and 'kernel' in param_specific_state.mu['dense_layer'], \"Adam 'mu' state for kernel not found.\"\n",
        "    print(\"\\nOptimizer state structure looks correct for Adam.\")\n",
        "else:\n",
        "    print(\"\\nWarning: Optimizer state structure for Adam might be different or not fully verified.\")"
      ],
      "metadata": {
        "id": "f1ccATgB-Zed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 7: Training Step with Flax NNX and Optax\n",
        "\n",
        "**Goal**: Implement a complete JIT-compiled training step for a Flax NNX model using Optax.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. You'll need:\n",
        " - An instance of your model class (e.g., my_model_sol from Ex 5/6 solution).\n",
        " - An instance of nnx.Optimizer (e.g., nnx_optimizer_sol from Ex 6 solution).\n",
        "2. Define a train_step function that is decorated with @nnx.jit. This function should take the model, optimizer, input x_batch, and target y_batch as arguments.\n",
        "3. Inside train_step:\n",
        " - Define an inner loss_fn_for_grad. This function must take the model as its first argument. Inside, it computes the model's predictions for x_batch and then calculates the mean squared error (MSE) against y_batch.\n",
        " - Use nnx.value_and_grad(loss_fn_for_grad)(model_arg) to compute both the loss value and the gradients with respect to the model passed to loss_fn_for_grad. (model_arg is the model instance passed into train_step).\n",
        " - Update the model's parameters (and the optimizer's state) using optimizer_arg.update(model_arg, grads). The update method takes the model and gradients, and updates the model's state in-place.\n",
        " - Return the computed loss_value.\n",
        "4. Create dummy x_batch and y_batch data.\n",
        "5. Call your train_step function. Print the returned loss.\n",
        "6. (Optional) Verify that the model's parameters have changed after the train_step by comparing a parameter value before and after the call."
      ],
      "metadata": {
        "id": "i7jXowc9ACNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 7\n",
        "key_ex7_main, main_key = jax.random.split(main_key)\n",
        "key_ex7_x, key_ex7_y = jax.random.split(key_ex7_main, 2)\n",
        "\n",
        "# 1. Use model and optimizer from previous exercises' solutions\n",
        "# Ensure my_model_sol and nnx_optimizer_sol are available\n",
        "if 'my_model_sol' not in globals() or 'nnx_optimizer_sol' not in globals():\n",
        "    print(\"Re-initializing model and optimizer from Ex5/Ex6 solutions for Ex7.\")\n",
        "    key_ex7_model_fallback, main_key = jax.random.split(main_key)\n",
        "    _model_din_ex7 = 3\n",
        "    _model_dout_ex7 = 2\n",
        "    _model_rngs_ex7 = nnx.Rngs(params=key_ex7_model_fallback)\n",
        "    # Ensure SimpleNNXModel_Sol is used\n",
        "    my_model_ex7 = SimpleNNXModel_Sol(din=_model_din_ex7, dout=_model_dout_ex7, rngs=_model_rngs_ex7)\n",
        "    _optax_tx_ex7 = optax.adam(learning_rate=0.001)\n",
        "    nnx_optimizer_ex7 = nnx.Optimizer(my_model_ex7, _optax_tx_ex7)\n",
        "    print(\"Model and optimizer re-created for Ex7.\")\n",
        "else:\n",
        "    my_model_ex7 = my_model_sol\n",
        "    nnx_optimizer_ex7 = nnx_optimizer_sol\n",
        "    print(\"Using 'my_model_sol' and 'nnx_optimizer_sol' for 'my_model_ex7' and 'nnx_optimizer_ex7'.\")\n",
        "\n",
        "\n",
        "# 2. & 3. Define the train_step function\n",
        "# TODO: Decorate with @nnx.jit\n",
        "# def train_step(model_arg: nnx.Module, optimizer_arg: nnx.Optimizer, # Type hint with base nnx.Module\n",
        "#                x_batch: jnp.ndarray, y_batch: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "    # TODO: Define inner loss_fn_for_grad(current_model_state_for_grad_fn)\n",
        "    # def loss_fn_for_grad(model_in_grad_fn: nnx.Module): # Type hint with base nnx.Module\n",
        "        # y_pred = model_in_grad_fn(x_batch)\n",
        "        # loss = jnp.mean((y_pred - y_batch)**2)\n",
        "        # return loss\n",
        "    #    return jnp.array(0.0) # Placeholder\n",
        "\n",
        "    # TODO: Compute loss value and gradients using nnx.value_and_grad\n",
        "    # loss_value, grads = nnx.value_and_grad(loss_fn_for_grad)(model_arg) # Pass model_arg\n",
        "\n",
        "    # TODO: Update the optimizer (which updates the model_arg in-place)\n",
        "    # optimizer_arg.update(model_arg, grads)\n",
        "\n",
        "    # return loss_value\n",
        "#    return jnp.array(0.0) # Placeholder defined train_step function\n",
        "\n",
        "# For the student to define:\n",
        "# Make sure the function signature is correct for nnx.jit\n",
        "@nnx.jit\n",
        "def train_step(model_arg: nnx.Module, optimizer_arg: nnx.Optimizer,\n",
        "               x_batch: jnp.ndarray, y_batch: jnp.ndarray) -> jnp.ndarray:\n",
        "    # Placeholder implementation for student\n",
        "    def loss_fn_for_grad(model_in_grad_fn: nnx.Module):\n",
        "        # y_pred = model_in_grad_fn(x_batch)\n",
        "        # loss = jnp.mean((y_pred - y_batch)**2)\n",
        "        # return loss\n",
        "        return jnp.array(0.0) # Student TODO: replace this\n",
        "\n",
        "    # loss_value, grads = nnx.value_and_grad(loss_fn_for_grad)(model_arg)\n",
        "    # optimizer_arg.update(grads)\n",
        "    # return loss_value\n",
        "    return jnp.array(-1.0) # Student TODO: replace this\n",
        "\n",
        "\n",
        "# 4. Create dummy data\n",
        "batch_s = 8\n",
        "# Access features_in and features_out carefully\n",
        "_din_from_model_ex7 = my_model_ex7.dense_layer.in_features if hasattr(my_model_ex7, 'dense_layer') else 3\n",
        "_dout_from_model_ex7 = my_model_ex7.dense_layer.out_features if hasattr(my_model_ex7, 'dense_layer') else 2\n",
        "\n",
        "x_batch_data = jax.random.normal(key_ex7_x, (batch_s, _din_from_model_ex7))\n",
        "y_batch_data = jax.random.normal(key_ex7_y, (batch_s, _dout_from_model_ex7))\n",
        "\n",
        "# Optional: Store initial param value for comparison\n",
        "initial_kernel_val = None\n",
        "if hasattr(my_model_ex7, 'get_state'):\n",
        "    _current_model_state_ex7 = my_model_ex7.get_state()\n",
        "    if 'dense_layer' in _current_model_state_ex7:\n",
        "       initial_kernel_val = _current_model_state_ex7['dense_layer']['kernel'].value[0,0].copy()\n",
        "print(f\"Initial kernel value (sample): {initial_kernel_val}\")\n",
        "\n",
        "# 5. Call the train_step\n",
        "# loss_after_step = train_step(my_model_ex7, nnx_optimizer_ex7, x_batch_data, y_batch_data) # Student will uncomment\n",
        "loss_after_step = jnp.array(-1.0) # Placeholder until student implements train_step\n",
        "if train_step(my_model_ex7, nnx_optimizer_ex7, x_batch_data, y_batch_data).item() != -1.0: # Check if student implemented\n",
        "    loss_after_step = train_step(my_model_ex7, nnx_optimizer_ex7, x_batch_data, y_batch_data)\n",
        "    print(f\"Loss after one training step: {loss_after_step}\")\n",
        "else:\n",
        "    print(\"Student needs to implement `train_step` function.\")\n",
        "\n",
        "\n",
        "# # 6. Optional: Verify parameter change\n",
        "# updated_kernel_val_sol = None\n",
        "# _, updated_model_state_sol = nnx.split(my_model_sol_ex7) # Get state again after update\n",
        "# if 'dense_layer' in updated_model_state_sol:\n",
        "#   updated_kernel_val_sol = updated_model_state_sol['dense_layer']['kernel'].value[0,0]\n",
        "#   print(f\"Updated kernel value (sample): {updated_kernel_val_sol}\")\n",
        "\n",
        "# if initial_kernel_val_sol is not None and updated_kernel_val_sol is not None:\n",
        "#     assert not jnp.allclose(initial_kernel_val_sol, updated_kernel_val_sol), \"Kernel parameter did not change!\"\n",
        "#     print(\"Kernel parameter changed as expected after the training step.\")\n",
        "# else:\n",
        "#     print(\"Could not verify kernel change (initial or updated value was None).\")"
      ],
      "metadata": {
        "id": "KEQCcmBI-ce2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 7: Training Step with Flax NNX and Optax\n",
        "key_ex7_sol_main, main_key = jax.random.split(main_key)\n",
        "key_ex7_sol_x, key_ex7_sol_y = jax.random.split(key_ex7_sol_main, 2)\n",
        "\n",
        "# 1. Use model and optimizer from previous exercises' solutions\n",
        "if 'my_model_sol' not in globals() or 'nnx_optimizer_sol' not in globals():\n",
        "    print(\"Re-initializing model and optimizer from Ex5/Ex6 solutions for Ex7 solution.\")\n",
        "    key_ex7_sol_model_fallback, main_key = jax.random.split(main_key)\n",
        "    _model_din_sol_ex7 = 3\n",
        "    _model_dout_sol_ex7 = 2\n",
        "    _model_rngs_sol_ex7 = nnx.Rngs(params=key_ex7_sol_model_fallback)\n",
        "    # Ensure SimpleNNXModel_Sol is used for the solution\n",
        "    my_model_sol_ex7 = SimpleNNXModel_Sol(din=_model_din_sol_ex7, dout=_model_dout_sol_ex7, rngs=_model_rngs_sol_ex7)\n",
        "    _optax_tx_sol_ex7 = optax.adam(learning_rate=0.001)\n",
        "    nnx_optimizer_sol_ex7 = nnx.Optimizer(my_model_sol_ex7, _optax_tx_sol_ex7)\n",
        "    print(\"Model and optimizer re-created for Ex7 solution.\")\n",
        "else:\n",
        "    # If solutions are run sequentially, these will be the correct instances\n",
        "    my_model_sol_ex7 = my_model_sol\n",
        "    nnx_optimizer_sol_ex7 = nnx_optimizer_sol\n",
        "    print(\"Using 'my_model_sol' and 'nnx_optimizer_sol' for Ex7 solution.\")\n",
        "\n",
        "\n",
        "# 2. & 3. Define the train_step function\n",
        "@nnx.jit # Decorate with @nnx.jit for JIT compilation\n",
        "def train_step_sol(model_arg: nnx.Module, optimizer_arg: nnx.Optimizer, # Use base nnx.Module for generality\n",
        "                   x_batch: jnp.ndarray, y_batch: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "    # Define inner loss_fn_for_grad. It takes the model as its first argument.\n",
        "    # It captures x_batch and y_batch from the outer scope.\n",
        "    def loss_fn_for_grad(model_in_grad_fn: nnx.Module): # Use base nnx.Module\n",
        "        y_pred = model_in_grad_fn(x_batch) # Use the model passed to this inner function\n",
        "        loss = jnp.mean((y_pred - y_batch)**2)\n",
        "        return loss\n",
        "\n",
        "    # Compute loss value and gradients using nnx.value_and_grad.\n",
        "    # This will differentiate loss_fn_for_grad with respect to its first argument (model_in_grad_fn).\n",
        "    # We pass the current state of our model (model_arg) to it.\n",
        "    loss_value, grads = nnx.value_and_grad(loss_fn_for_grad)(model_arg)\n",
        "\n",
        "    # Update the optimizer. This updates the model_arg (which nnx_optimizer_sol_ex7 references) in-place.\n",
        "    optimizer_arg.update(model_arg, grads)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "# 4. Create dummy data\n",
        "batch_s_sol = 8\n",
        "# Ensure din and dout match the model instantiation from Ex5/Ex6\n",
        "# my_model_sol_ex7.dense_layer is an nnx.Linear object\n",
        "din_from_model_sol = my_model_sol_ex7.dense_layer.in_features\n",
        "dout_from_model_sol = my_model_sol_ex7.dense_layer.out_features\n",
        "\n",
        "x_batch_data_sol = jax.random.normal(key_ex7_sol_x, (batch_s_sol, din_from_model_sol))\n",
        "y_batch_data_sol = jax.random.normal(key_ex7_sol_y, (batch_s_sol, dout_from_model_sol))\n",
        "\n",
        "# Optional: Store initial param value for comparison\n",
        "initial_kernel_val_sol = None\n",
        "_, current_model_state_sol = nnx.split(my_model_sol_ex7)\n",
        "if 'dense_layer' in current_model_state_sol:\n",
        "    initial_kernel_val_sol = current_model_state_sol['dense_layer']['kernel'].value[0,0].copy()\n",
        "print(f\"Initial kernel value (sample): {initial_kernel_val_sol}\")\n",
        "\n",
        "\n",
        "# 5. Call the train_step\n",
        "# First call will JIT compile the train_step_sol function.\n",
        "loss_after_step_sol = train_step_sol(my_model_sol_ex7, nnx_optimizer_sol_ex7, x_batch_data_sol, y_batch_data_sol)\n",
        "print(f\"Loss after one training step (1st call, JIT): {loss_after_step_sol}\")\n",
        "# Second call to show it's faster (though %timeit is better for measurement)\n",
        "loss_after_step_sol_2 = train_step_sol(my_model_sol_ex7, nnx_optimizer_sol_ex7, x_batch_data_sol, y_batch_data_sol)\n",
        "print(f\"Loss after one training step (2nd call, cached): {loss_after_step_sol_2}\")\n",
        "\n",
        "\n",
        "# 6. Optional: Verify parameter change\n",
        "updated_kernel_val_sol = None\n",
        "_, updated_model_state_sol = nnx.split(my_model_sol_ex7) # Get state again after update\n",
        "if 'dense_layer' in updated_model_state_sol:\n",
        "  updated_kernel_val_sol = updated_model_state_sol['dense_layer']['kernel'].value[0,0]\n",
        "  print(f\"Updated kernel value (sample): {updated_kernel_val_sol}\")\n",
        "\n",
        "if initial_kernel_val_sol is not None and updated_kernel_val_sol is not None:\n",
        "    assert not jnp.allclose(initial_kernel_val_sol, updated_kernel_val_sol), \"Kernel parameter did not change!\"\n",
        "    print(\"Kernel parameter changed as expected after the training step.\")\n",
        "else:\n",
        "    print(\"Could not verify kernel change (initial or updated value was None).\")"
      ],
      "metadata": {
        "id": "7bVlg9_-Ae6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 8: Orbax - Saving and Restoring Checkpoints\n",
        "\n",
        "**Goal**: Learn to use Orbax to save and restore JAX PyTrees, specifically Flax NNX model states and Optax optimizer states.\n",
        "\n",
        "### Instructions:\n",
        "1. You'll need your model (e.g., my_model_sol_ex7) and optimizer (e.g., nnx_optimizer_sol_ex7) from the previous exercise's solution.\n",
        "2. Define a checkpoint directory (e.g., /tmp/my_nnx_checkpoint/).\n",
        "3. Create an Orbax CheckpointManagerOptions and then a CheckpointManager.\n",
        "4. Bundle the states you want to save into a dictionary. For NNX, this is my_model_sol_ex7.get_state() for the model, and nnx_optimizer_sol_ex7.state for the optimizer's internal state. Also include a training step counter.\n",
        "5. Use checkpoint_manager.save() with ocp.args.StandardSave() to save the bundled state. Call checkpoint_manager.wait_until_finished() to ensure saving completes.\n",
        "6. To restore:\n",
        " - Create new instances of your model (restored_model) and Optax transform (restored_optax_tx). The new model should have a different PRNG key for its initial parameters to demonstrate that restoration works.\n",
        " - Use checkpoint_manager.restore() with ocp.args.StandardRestore() to load the bundled state.\n",
        " - Apply the loaded model state to restored_model using restored_model.update_state(loaded_bundle['model']).\n",
        " - Create a new nnx.Optimizer (restored_optimizer) associating restored_model and restored_optax_tx.\n",
        " - Assign the loaded optimizer state to the new optimizer: restored_optimizer.state = loaded_bundle['optimizer'].\n",
        "7. Verify that a parameter from restored_model matches the corresponding parameter from the original my_model_sol_ex7 (before saving, or from the saved state). Also, compare optimizer states if possible.\n",
        "8. Clean up the checkpoint directory."
      ],
      "metadata": {
        "id": "_t8KGFhqDoSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions for Exercise 8\n",
        "# import orbax.checkpoint as ocp # Already imported\n",
        "# import os, shutil # Already imported\n",
        "\n",
        "# 1. Use model and optimizer from previous exercise solution\n",
        "if 'my_model_sol_ex7' not in globals() or 'nnx_optimizer_sol_ex7' not in globals():\n",
        "    print(\"Re-initializing model and optimizer from Ex7 solution for Ex8.\")\n",
        "    key_ex8_model_fallback, main_key = jax.random.split(main_key)\n",
        "    _model_din_ex8 = 3\n",
        "    _model_dout_ex8 = 2\n",
        "    _model_rngs_ex8 = nnx.Rngs(params=key_ex8_model_fallback)\n",
        "    _ModelClassEx8 = SimpleNNXModel_Sol if 'SimpleNNXModel_Sol' in globals() else SimpleNNXModel\n",
        "    model_to_save = _ModelClassEx8(din=_model_din_ex8, dout=_model_dout_ex8, rngs=_model_rngs_ex8)\n",
        "    _optax_tx_ex8 = optax.adam(learning_rate=0.001)\n",
        "    optimizer_to_save = nnx.Optimizer(model_to_save, _optax_tx_ex8)\n",
        "    print(\"Model and optimizer re-created for Ex8.\")\n",
        "else:\n",
        "    model_to_save = my_model_sol_ex7\n",
        "    optimizer_to_save = nnx_optimizer_sol_ex7\n",
        "    print(\"Using model and optimizer from Ex7 solution for Ex8.\")\n",
        "\n",
        "# 2. Define checkpoint directory\n",
        "# TODO: Define checkpoint_dir\n",
        "checkpoint_dir = None # Placeholder e.g., \"/tmp/my_nnx_checkpoint_exercise/\"\n",
        "# if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "#    shutil.rmtree(checkpoint_dir) # Clean up previous runs for safety\n",
        "# if checkpoint_dir:\n",
        "#    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# 3. Create Orbax CheckpointManager\n",
        "# TODO: Create options and manager\n",
        "# options = ocp.CheckpointManagerOptions(...)\n",
        "# mngr = ocp.CheckpointManager(...)\n",
        "options = None\n",
        "mngr = None\n",
        "\n",
        "# 4. Bundle states\n",
        "# current_step = 100 # Example step\n",
        "# TODO: Get model_state and optimizer_state\n",
        "# model_state_to_save = nnx.split(model_to_save)\n",
        "# The optimizer state is now accessed via the .state attribute.\n",
        "# opt_state_to_save = optimizer_to_save.state\n",
        "# save_bundle = {\n",
        "#     'model': model_state_to_save,\n",
        "#     'optimizer': opt_state_to_save,\n",
        "#     'step': current_step\n",
        "# }\n",
        "save_bundle = None\n",
        "\n",
        "# 5. Save the checkpoint\n",
        "# if mngr and save_bundle:\n",
        "#   TODO: Save checkpoint\n",
        "#   mngr.save(...)\n",
        "#   mngr.wait_until_finished()\n",
        "#   print(f\"Checkpoint saved at step {current_step} to {checkpoint_dir}\")\n",
        "# else:\n",
        "#   print(\"Checkpoint manager or save_bundle not initialized.\")\n",
        "\n",
        "# --- Restoration ---\n",
        "# 6.a Create new model and Optax transform (for restoration)\n",
        "# key_ex8_restore_model, main_key = jax.random.split(main_key)\n",
        "# din_restore = model_to_save.dense_layer.in_features if hasattr(model_to_save, 'dense_layer') else 3\n",
        "# dout_restore = model_to_save.dense_layer.out_features if hasattr(model_to_save, 'dense_layer') else 2\n",
        "# _ModelClassRestore = SimpleNNXModel_Sol if 'SimpleNNXModel_Sol' in globals() else SimpleNNXModel\n",
        "# restored_model = _ModelClassRestore(\n",
        "#     din=din_restore, dout=dout_restore,\n",
        "#     rngs=nnx.Rngs(params=key_ex8_restore_model) # New key for different initial params\n",
        "# )\n",
        "# restored_optax_tx = optax.adam(learning_rate=0.001) # Same Optax config\n",
        "restored_model = None\n",
        "restored_optax_tx = None\n",
        "\n",
        "# 6.b Restore the checkpoint\n",
        "# loaded_bundle = None\n",
        "# if mngr:\n",
        "#   TODO: Restore checkpoint\n",
        "#   latest_step = mngr.latest_step()\n",
        "#   if latest_step is not None:\n",
        "#       loaded_bundle = mngr.restore(...)\n",
        "#       print(f\"Checkpoint restored from step {latest_step}\")\n",
        "#   else:\n",
        "#       print(\"No checkpoint found to restore.\")\n",
        "# else:\n",
        "#   print(\"Checkpoint manager not initialized for restore.\")\n",
        "\n",
        "# 6.c Apply loaded states\n",
        "# if loaded_bundle and restored_model:\n",
        "#   TODO: Update restored_model state\n",
        "#   nnx.update(restored_model, ...)\n",
        "#   print(\"Restored model state applied.\")\n",
        "\n",
        "    # TODO: Create new nnx.Optimizer and assign its state\n",
        "#   restored_optimizer = nnx.Optimizer(...)\n",
        "#   restored_optimizer.state = ...\n",
        "#   print(\"Restored optimizer state applied.\")\n",
        "# else:\n",
        "#   print(\"Loaded_bundle or restored_model is None, cannot apply states.\")\n",
        "restored_optimizer = None\n",
        "\n",
        "# 7. Verify restoration\n",
        "# original_kernel_sol = save_bundle_sol['model']['dense_layer']['kernel']\n",
        "# _, restored_model_state = nnx.split(restored_model_sol)\n",
        "# kernel_after_restore_sol = restored_model_state['dense_layer']['kernel']\n",
        "# assert jnp.array_equal(original_kernel_sol.value, kernel_after_restore_sol.value), \\\n",
        "#        \"Model kernel parameters differ after restoration!\"\n",
        "# print(\"\\nModel parameters successfully restored and verified (kernel match).\")\n",
        "\n",
        "# # Verify optimizer state (e.g., Adam's 'mu' for a specific parameter)\n",
        "# original_opt_state_adam_mu_kernel = save_bundle_sol['optimizer'][0].mu['dense_layer']['kernel'].value\n",
        "# restored_opt_state_adam_mu_kernel = restored_optimizer_sol.state[0].mu['dense_layer']['kernel'].value\n",
        "# assert jnp.array_equal(original_opt_state_adam_mu_kernel, restored_opt_state_adam_mu_kernel), \\\n",
        "#                        \"Optimizer Adam mu for kernel differs!\"\n",
        "# print(\"Optimizer state (sample mu) successfully restored and verified.\")\n",
        "\n",
        "\n",
        "# 8. Clean up\n",
        "# if mngr:\n",
        "#   mngr.close()\n",
        "# if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "#   shutil.rmtree(checkpoint_dir)\n",
        "#   print(f\"Cleaned up checkpoint directory: {checkpoint_dir}\")"
      ],
      "metadata": {
        "id": "V7XdNy-vAjpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution 8: Orbax - Saving and Restoring Checkpoints\n",
        "\n",
        "# 1. Use model and optimizer from previous exercise solution\n",
        "if 'my_model_sol_ex7' not in globals() or 'nnx_optimizer_sol_ex7' not in globals():\n",
        "    print(\"Re-initializing model and optimizer from Ex7 solution for Ex8 solution.\")\n",
        "    key_ex8_sol_model_fallback, main_key = jax.random.split(main_key)\n",
        "    _model_din_sol_ex8 = 3\n",
        "    _model_dout_sol_ex8 = 2\n",
        "    _model_rngs_sol_ex8 = nnx.Rngs(params=key_ex8_sol_model_fallback)\n",
        "    # Ensure SimpleNNXModel_Sol is used for the solution\n",
        "    model_to_save_sol = SimpleNNXModel_Sol(din=_model_din_sol_ex8,\n",
        "                                           dout=_model_dout_sol_ex8,\n",
        "                                           rngs=_model_rngs_sol_ex8)\n",
        "    _optax_tx_sol_ex8 = optax.adam(learning_rate=0.001) # Store the transform for later\n",
        "    optimizer_to_save_sol = nnx.Optimizer(model_to_save_sol, _optax_tx_sol_ex8)\n",
        "    print(\"Model and optimizer re-created for Ex8 solution.\")\n",
        "else:\n",
        "    model_to_save_sol = my_model_sol_ex7\n",
        "    optimizer_to_save_sol = nnx_optimizer_sol_ex7\n",
        "    # We need the optax transform used to create the optimizer for restoration\n",
        "    _optax_tx_sol_ex8 = optimizer_to_save_sol.tx # Access the original Optax transform\n",
        "    print(\"Using model and optimizer from Ex7 solution for Ex8 solution.\")\n",
        "\n",
        "# 2. Define checkpoint directory\n",
        "checkpoint_dir_sol = \"/tmp/my_nnx_checkpoint_exercise_solution/\"\n",
        "if os.path.exists(checkpoint_dir_sol):\n",
        "   shutil.rmtree(checkpoint_dir_sol) # Clean up previous runs\n",
        "os.makedirs(checkpoint_dir_sol, exist_ok=True)\n",
        "print(f\"Orbax checkpoint directory: {checkpoint_dir_sol}\")\n",
        "\n",
        "# 3. Create Orbax CheckpointManager\n",
        "options_sol = ocp.CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1)\n",
        "mngr_sol = ocp.CheckpointManager(checkpoint_dir_sol, options=options_sol)\n",
        "\n",
        "# 4. Bundle states\n",
        "current_step_sol = 100 # Example step\n",
        "_, model_state_to_save_sol = nnx.split(model_to_save_sol)\n",
        "# The optimizer state is now a PyTree directly available in the .state attribute.\n",
        "opt_state_to_save_sol = optimizer_to_save_sol.opt_state\n",
        "save_bundle_sol = {\n",
        "    'model': model_state_to_save_sol,\n",
        "    'optimizer': opt_state_to_save_sol,\n",
        "    'step': current_step_sol\n",
        "}\n",
        "print(\"\\nState bundle to be saved:\")\n",
        "pprint.pprint(f\"Model state keys: {model_state_to_save_sol.keys()}\")\n",
        "pprint.pprint(f\"Optimizer state type: {type(opt_state_to_save_sol)}\")\n",
        "\n",
        "\n",
        "# 5. Save the checkpoint\n",
        "mngr_sol.save(current_step_sol, args=ocp.args.StandardSave(save_bundle_sol))\n",
        "mngr_sol.wait_until_finished()\n",
        "print(f\"\\nCheckpoint saved at step {current_step_sol} to {checkpoint_dir_sol}\")\n",
        "\n",
        "# --- Restoration ---\n",
        "# 6.a Create new model and Optax transform (for restoration)\n",
        "key_ex8_sol_restore_model, main_key = jax.random.split(main_key)\n",
        "# Ensure din/dout are correctly obtained from the saved model's structure if possible\n",
        "# Assuming model_to_save_sol is SimpleNNXModel_Sol which has a dense_layer\n",
        "din_restore_sol = model_to_save_sol.dense_layer.in_features\n",
        "dout_restore_sol = model_to_save_sol.dense_layer.out_features\n",
        "\n",
        "restored_model_sol = SimpleNNXModel_Sol( # Use the solution's model class\n",
        "    din=din_restore_sol, dout=dout_restore_sol,\n",
        "    rngs=nnx.Rngs(params=key_ex8_sol_restore_model) # New key for different initial params\n",
        ")\n",
        "# We need the original Optax transform definition for the new nnx.Optimizer\n",
        "# _optax_tx_sol_ex8 was stored earlier, or can be re-created if config is known\n",
        "restored_optax_tx_sol = _optax_tx_sol_ex8\n",
        "\n",
        "# Print a param from new model BEFORE restoration to show it's different\n",
        "_, kernel_before_restore_sol = nnx.split(restored_model_sol)\n",
        "print(f\"\\nSample kernel from 'restored_model_sol' BEFORE restoration:\")\n",
        "nnx.display(kernel_before_restore_sol['dense_layer']['kernel'])\n",
        "\n",
        "# 6.b Restore the checkpoint\n",
        "loaded_bundle_sol = None\n",
        "latest_step_sol = mngr_sol.latest_step()\n",
        "if latest_step_sol is not None:\n",
        "    # For NNX, we are restoring raw PyTrees, StandardRestore is suitable.\n",
        "    loaded_bundle_sol = mngr_sol.restore(latest_step_sol,\n",
        "                                         args=ocp.args.StandardRestore(save_bundle_sol))\n",
        "    print(f\"\\nCheckpoint restored from step {latest_step_sol}\")\n",
        "    print(f\"Loaded bundle contains keys: {loaded_bundle_sol.keys()}\")\n",
        "else:\n",
        "    raise ValueError(\"No checkpoint found to restore.\")\n",
        "\n",
        "# 6.c Apply loaded states\n",
        "assert loaded_bundle_sol is not None, \"Loaded bundle is None\"\n",
        "nnx.update(restored_model_sol, loaded_bundle_sol['model'])\n",
        "print(\"Restored model state applied to 'restored_model_sol'.\")\n",
        "\n",
        "# Create new nnx.Optimizer with the restored_model and original optax_tx\n",
        "restored_optimizer_sol = nnx.Optimizer(restored_model_sol, restored_optax_tx_sol,\n",
        "                                       wrt=nnx.Param)\n",
        "# Now assign the loaded Optax state PyTree\n",
        "restored_optimizer_sol.state = loaded_bundle_sol['optimizer']\n",
        "print(\"Restored optimizer state applied to 'restored_optimizer_sol'.\")\n",
        "\n",
        "\n",
        "# 7. Verify restoration\n",
        "original_kernel_sol = save_bundle_sol['model']['dense_layer']['kernel']\n",
        "_, restored_model_state = nnx.split(restored_model_sol)\n",
        "kernel_after_restore_sol = restored_model_state['dense_layer']['kernel']\n",
        "assert jnp.array_equal(original_kernel_sol.value, kernel_after_restore_sol.value), \\\n",
        "       \"Model kernel parameters differ after restoration!\"\n",
        "print(\"\\nModel parameters successfully restored and verified (kernel match).\")\n",
        "\n",
        "# Verify optimizer state (e.g., Adam's 'mu' for a specific parameter)\n",
        "original_opt_state_adam_mu_kernel = save_bundle_sol['optimizer'][0].mu['dense_layer']['kernel'].value\n",
        "restored_opt_state_adam_mu_kernel = restored_optimizer_sol.state[0].mu['dense_layer']['kernel'].value\n",
        "assert jnp.array_equal(original_opt_state_adam_mu_kernel, restored_opt_state_adam_mu_kernel), \\\n",
        "                       \"Optimizer Adam mu for kernel differs!\"\n",
        "print(\"Optimizer state (sample mu) successfully restored and verified.\")\n",
        "\n",
        "\n",
        "# 8. Clean up\n",
        "mngr_sol.close()\n",
        "if os.path.exists(checkpoint_dir_sol):\n",
        "  shutil.rmtree(checkpoint_dir_sol)\n",
        "  print(f\"Cleaned up checkpoint directory: {checkpoint_dir_sol}\")"
      ],
      "metadata": {
        "id": "2-Fk8aukEGVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "### Congratulations on completing the JAX AI Stack exercises!\n",
        "\n",
        "You've now had a hands-on introduction to:\n",
        "\n",
        "- Core JAX: jax.numpy, functional programming, jax.jit, jax.grad, jax.vmap.\n",
        "- Flax NNX: Defining and instantiating Pythonic neural network models.\n",
        "- Optax: Creating and using composable optimizers with Flax NNX.\n",
        "- Training Loop: Implementing an end-to-end training step in Flax NNX.\n",
        "- Orbax: Saving and restoring model and optimizer states.\n",
        "\n",
        "This forms a strong foundation for developing high-performance machine learning models with the JAX ecosystem.\n",
        "\n",
        "For further learning, refer to the official documentation:\n",
        "- JAX AI Stack: https://jaxstack.ai\n",
        "- JAX: https://jax.dev\n",
        "- Flax NNX: https://flax.readthedocs.io\n",
        "- Optax: https://optax.readthedocs.io\n",
        "- Orbax: https://orbax.readthedocs.io\n",
        "\n",
        "Don't forget to provide feedback on the training session:\n",
        "https://goo.gle/jax-training-feedback"
      ],
      "metadata": {
        "id": "9kotBqE7Qhiv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdQIp5G9QqwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}